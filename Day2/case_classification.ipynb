{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Started\n",
    "Breast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a results of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound and biopsy are commonly used to diagnose breast cancer performed.\n",
    "\n",
    "Given breast cancer results from breast fine needle aspiration (FNA) test (is a quick and simple procedure to perform, which removes some fluid or cells from a breast lesion or cyst (a lump, sore or swelling) with a fine needle similar to a blood sample needle). Since this build a model that can classify a breast cancer tumor using two training classification:\n",
    "\n",
    "    1= Malignant (Cancerous) - Present\n",
    "    0= Benign (Not Cancerous) -Absent\n",
    "\n",
    "\n",
    "The Breast Cancer datasets is available machine learning repository maintained by the University of California, Irvine. The dataset contains 569 samples of malignant and benign tumor cells.\n",
    "\n",
    "+ The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively.\n",
    "+ The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant.\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "## Objective:\n",
    "\n",
    "The repository is a learning exercise to:\n",
    "\n",
    "- Apply the fundamental concepts of machine learning from an available dataset\n",
    "\n",
    "The analysis is divided into four sections, saved in juypter notebooks in this repository\n",
    "\n",
    "1. Identifying the problem and Data Sources\n",
    "2. Data Explorations\n",
    "3. Data Pre-Processing\n",
    "4. Model Development\n",
    "5. Model Improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check Environments version \n",
    "# --------------------------\n",
    "# Python version\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "# scipy\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "# numpy\n",
    "import numpy\n",
    "print('numpy: {}'.format(numpy.__version__))\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "# pandas\n",
    "import pandas\n",
    "print('pandas: {}'.format(pandas.__version__))\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: {}'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------\n",
    "# 1.  Identifying the problem and Getting data.\n",
    "\n",
    "Identify the types of information contained in our data set In this notebook I used Python modules to import external data sets for the purpose of getting to know/familiarize myself with the data to get a good grasp of the data and think about how to handle the data in different ways. \n",
    "\n",
    "--------------------------------------------------------\n",
    "\n",
    "### A. Import library\n",
    "\n",
    "First, let’s import all/ minimum of the modules, functions and objects we are going to use in this tutorial\n",
    "\n",
    "\n",
    "\n",
    "### B. Load Dataset\n",
    "\n",
    "First, load the supplied CSV file using additional options in the Pandas read_csv function.\n",
    "Inspecting the data or We can load the data directly from the UCI Machine Learning repository in sklearn.\n",
    "\n",
    "The first step is to visually inspect the new data set. There are multiple ways to achieve this:\n",
    "\n",
    "* The easiest being to request the first few records using the DataFrame data.head()* method. By default, “data.head()” returns the first 5 rows from the DataFrame object df (excluding the header row).\n",
    "* Alternatively, one can also use “df.tail()” to return the five rows of the data frame.\n",
    "* For both head and tail methods, there is an option to specify the number of records by including the required number in between the parentheses when calling either method.Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A. Import library \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# B. Load Dataset\n",
    "dataset=load_breast_cancer()\n",
    "print dataset.DESCR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "# 2. Data Explorations\n",
    "\n",
    "Goal : Explore the variables to assess how they relate to the response variable In this notebook, I am getting familiar with the data using data exploration and visualization techniques using python libraries (Pandas, matplotlib, seaborn. Familiarity with the data is important which will provide useful knowledge for data pre-processing)\n",
    "\n",
    "\n",
    "### Statistical Summary\n",
    "\n",
    "Now we can take a look at a summary of each attribute.\n",
    "\n",
    "This includes the count, mean, the min and max values as well as some percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we use pandas as container the data\n",
    "# set to pandas\n",
    "pd_dataset=pd.DataFrame(dataset.data)\n",
    "pd_dataset.columns= dataset.feature_names\n",
    "\n",
    "# global name X,y for simple name\n",
    "X= pd_dataset\n",
    "y=dataset.target\n",
    "print pd_dataset.describe()\n",
    "print pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summirize dataset using matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "color_dic = {0:'red', 1:'blue'}\n",
    "#Plot histograms of CUT1 variables\n",
    "data_mean=pd_dataset.iloc[:,0:10]\n",
    "\n",
    "# colors = pd_dataset['class'].map(lambda x: color_dic.get(x))\n",
    "colors= [color_dic.get(c) for c in y]\n",
    "\n",
    "sm = pd.scatter_matrix(data_mean, c=colors, alpha=0.4, figsize=((15,15)));\n",
    "hist_mean=data_mean.hist(bins=10, figsize=(15, 10),grid=False,)\n",
    "\n",
    "#Density Plots\n",
    "data_mean.plot(kind= 'density', subplots=True, layout=(4,3), sharex=False, \n",
    "                     sharey=False,fontsize=12, figsize=(15,10))\n",
    "\n",
    "data_mean.plot(kind='box', subplots=True, layout=(4,3), sharex=False, sharey=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "----------------------------------------\n",
    "# 3. Data Preprocessing\n",
    "\n",
    "Data preprocessing is a crucial step for any data analysis problem. It is often a very good idea to prepare your data in such way to best expose the structure of the problem to the machine learning algorithms that you intend to use.This involves a number of activities such as:\n",
    "- Assigning numerical values to categorical data;\n",
    "- Handling missing values; and\n",
    "- Normalizing the features (so that features on small scales do not dominate when fitting a model to the data).\n",
    "\n",
    "Goal : Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model. \n",
    "\n",
    "*) We assume that the dataset for this project is clean. So we not do preprocessing right now\n",
    "\n",
    "  \n",
    "--------------------------------\n",
    "\n",
    "In this sections to improve the models, I use  some techniques, that are : (Ignoring when you try a simple way)\n",
    "1. feature normalization\n",
    "2. feature selection to reduce high-dimension data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Data pre-processing\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# data_scaler=StandardScaler()\n",
    "# Xs=data_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction using PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # feature selection and feature reduction\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# import numpy as np\n",
    "# # feature extraction\n",
    "# pca = PCA(n_components=10)\n",
    "# X_reduc = pca.fit_transform(Xs)\n",
    "\n",
    "# # summarize components\n",
    "# print(\"Explained Variance: %s\") % (sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# #The amount of variance that each PC explains\n",
    "# var= pca.explained_variance_ratio_\n",
    "# #Cumulative Variance explains\n",
    "# var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)              \n",
    "# print(var1)\n",
    "# X=X_reduc\n",
    "\n",
    "# # Deciding How Many Principal Components to Retain\n",
    "# # - Kaiser’s criterion,Elbow\n",
    "# # - screeplot\n",
    "# plt.plot(pca.explained_variance_)\n",
    "# plt.title('Scree Plot')\n",
    "# plt.xlabel('Principal Component')\n",
    "# plt.ylabel('Eigenvalue')\n",
    "\n",
    "# leg = plt.legend(['Eigenvalues from PCA'], loc='best', borderpad=0.3,shadow=False,markerscale=0.4)\n",
    "# leg.get_frame().set_alpha(0.4)\n",
    "# leg.draggable(state=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "----------------------------------------------------------\n",
    "# 4. Models Development \n",
    "\n",
    "In this section of the project, you will develop the tools and techniques necessary for models to make a prediction. Being able to make accurate evaluations of each model's performance through the use of these tools and techniques helps to greatly reinforce the confidence in your predictions.\n",
    "\n",
    "We don’t know which algorithms would be good on this problem or what configurations to use. We get an idea from the plots that some of the classes are partially linearly separable in some dimensions, so we are expecting generally good results.\n",
    "\n",
    "Let’s evaluate 3 different algorithms:\n",
    "- KNeighborsClassifier,\n",
    "- DecisionTreeClassifier,\n",
    "- RandomForestClassifier,\n",
    "- GaussianNB,     \n",
    "- SVM,\n",
    "\n",
    "\n",
    "----------------------------------------------------------\n",
    "### A. Split Data\n",
    "Before build the models, the next implementation requires that you take the dataset and split the data into training and testing subsets. Typically, the data is also shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset.\n",
    "\n",
    "For the code cell below, you will need to implement the following:\n",
    "\n",
    "- Use train_test_split from sklearn.cross_validation to shuffle and split the features and prices data into training and testing sets.\n",
    "- Split the data into 75% training and 25% testing.\n",
    "- Set the random_state for train_test_split to a value of your choice. This ensures results are consistent.\n",
    "    Assign the train and testing splits to X_train, X_test, y_train, and y_test.\n",
    "\n",
    "\n",
    "### B. Models Validation\n",
    "\n",
    "It is difficult to measure the quality of a given model without quantifying its performance over training and testing. This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement. For this project, we will be calculating the accuracy_score function to computes the accuracy. the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\n",
    "\n",
    "\n",
    "### C. Cross validation\n",
    "Cross Validation technique  assess the performance of machine learning models. It helps in knowing how the machine learning model would generalize to an independent data set. \n",
    "Tthis is one of resampling methods to make the best use of your training data in order to accurately estimate the performance of a model on new unseen data.\n",
    "\n",
    "\n",
    "Accurate estimates of performance can then be used to help you choose which set of model parameters to use or which model to select. Once you have chosen a model, you can train for final model on the entire training dataset and start using it to make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# initialization method classifier\n",
    "classifier=[\n",
    "        KNeighborsClassifier(5),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),        \n",
    "        GaussianNB(),             \n",
    "        SVC(probability=True, C=100)       \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "\n",
    "accuracy_all = []\n",
    "cvs_all = []\n",
    "names=[]\n",
    "\n",
    "for clf in classifier:\n",
    "    start = time.time()\n",
    "    name= clf.__class__.__name__\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    kfold = KFold(n_splits=10, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=kfold)\n",
    "    predict_score=accuracy_score(prediction, y_test)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Execution time: {0:.5} seconds \\n\".format(end-start))\n",
    "    \n",
    "    accuracy_all.append(predict_score)\n",
    "    cvs_all.append(np.mean(scores))\n",
    "    names.append(name)\n",
    "    \n",
    "    \n",
    "    print(name, \"Classifier Accuracy: {0:.2%}\".format(predict_score))\n",
    "    print(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\n",
    "#     print (classification_report(prediction, y_test))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create report to pandas\n",
    "\n",
    "data={'accuracy': accuracy_all, 'cross val':cvs_all }\n",
    "pd_data=pd.DataFrame(data)\n",
    "pd_data.index=names\n",
    "print pd_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "# 5. Model Improvement\n",
    "\n",
    "\n",
    "Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model. \n",
    "In this project I use two sub step to improve the accuracy the best model, that are: \n",
    "\n",
    "1. Preprocessing : We use feature selection to reduce high-dimension data, feature extraction and transformation for dimensionality reduction\n",
    "2. parameters tuning  in order to find one with the best model's performance with best parameters ( hyper-parameters). \n",
    "\n",
    "Note : Machine learning models are parameterized so that their behavior can be tuned for a given problem.\n",
    "Models can have many parameters and finding the best combination of parameters can be treated as a search problem. Not all parameters of a classifier is learned from the estimators. Those parameters are called hyper-parameters and are passed as arguments to the constructor of the classifier. Each estimator has a different set of hyper-parameters, which can be found in the corresponding documentation.\n",
    "We can search for the best performance of the classifier sampling different hyper-parameter combinations. This will be done with an exhaustive grid search, provided by the GridSearchCV function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print('--------- Now Trying Support Vector Machine Classifier ---------')\n",
    "\n",
    "#Tune Hyperparameters\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [{'C': param_range,\n",
    "               'kernel': ['linear']},\n",
    "              {'C': param_range,\n",
    "               'gamma': param_range,\n",
    "               'kernel': ['rbf']}]\n",
    "clf = GridSearchCV(estimator= SVC(probability=True),\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10,\n",
    "                  n_jobs=1)\n",
    "\n",
    "prediction = clf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "\n",
    "print(\"Support Vector Machine Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\n",
    "print(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\n",
    "print(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))\n",
    "print(\"Best parameters: {0} \\n\".format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('--------- Now Trying Support Random Forest Classifier ---------')\n",
    "start = time.time()\n",
    "\n",
    "parameters = {'n_estimators':list(range(70,101)), 'criterion':['gini', 'entropy']}\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), parameters, scoring = 'accuracy', n_jobs=4)\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Random Forest Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\n",
    "print(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\n",
    "print(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))\n",
    "print(\"Best parameters: {0} \\n\".format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('--------- Now Trying Support Naive Bayes Classifier  ---------')\n",
    "start = time.time()\n",
    "\n",
    "parameters = {'priors':[[0.01, 0.99],[0.1, 0.9], [0.2, 0.8], [0.25, 0.75], [0.3, 0.7],[0.35, 0.65], [0.4, 0.6]]}\n",
    "\n",
    "clf = GridSearchCV(GaussianNB(), parameters, scoring = 'accuracy', n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\n",
    "print(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\n",
    "print(\"Execution time: {0:.5} seconds \\n\".format(end-start))\n",
    "print(\"Best parameters: {0}\".format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "### Example : Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(data_scaler, 'data_scaler_cancer.pkl')\n",
    "# joblib.dump(pca, 'pca_cancer.pkl')\n",
    "# joblib.dump(clf_svc, 'svc_cancer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # load model:\n",
    "# # 1.standard\n",
    "# # 2. pca\n",
    "# # 3. algorithm\n",
    "# svm= joblib.load('svc_cancer.pkl')\n",
    "# predict=svm.predict(X_test)\n",
    "# print 'Score :',accuracy_score(predict,y_test)\n",
    "\n",
    "# svm= joblib.load('lr_cancer.pkl')\n",
    "# predict=svm.predict(X_test)\n",
    "# print 'Score :',accuracy_score(predict,y_test)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
