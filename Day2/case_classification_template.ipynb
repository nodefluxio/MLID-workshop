{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Started\n",
    "Breast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a results of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound and biopsy are commonly used to diagnose breast cancer performed.\n",
    "\n",
    "Given breast cancer results from breast fine needle aspiration (FNA) test (is a quick and simple procedure to perform, which removes some fluid or cells from a breast lesion or cyst (a lump, sore or swelling) with a fine needle similar to a blood sample needle). Since this build a model that can classify a breast cancer tumor using two training classification:\n",
    "\n",
    "    1= Malignant (Cancerous) - Present\n",
    "    0= Benign (Not Cancerous) -Absent\n",
    "\n",
    "\n",
    "The Breast Cancer datasets is available machine learning repository maintained by the University of California, Irvine. The dataset contains 569 samples of malignant and benign tumor cells.\n",
    "\n",
    "+ The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively.\n",
    "+ The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant.\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "## Objective:\n",
    "\n",
    "The repository is a learning exercise to:\n",
    "\n",
    "- Apply the fundamental concepts of machine learning from an available dataset\n",
    "\n",
    "The analysis is divided into four sections, saved in juypter notebooks in this repository\n",
    "\n",
    "1. Identifying the problem and Data Sources\n",
    "2. Data Explorations\n",
    "3. Data Pre-Processing\n",
    "4. Model Development\n",
    "5. Model Improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "scipy: 1.0.0\n",
      "numpy: 1.13.3\n",
      "matplotlib: 2.0.2\n",
      "pandas: 0.21.0\n",
      "sklearn: 0.19.1\n"
     ]
    }
   ],
   "source": [
    "# check Environments version \n",
    "# --------------------------\n",
    "# Python version\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "# scipy\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "# numpy\n",
    "import numpy\n",
    "print('numpy: {}'.format(numpy.__version__))\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "# pandas\n",
    "import pandas\n",
    "print('pandas: {}'.format(pandas.__version__))\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: {}'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------\n",
    "# 1.  Identifying the problem and Getting data.\n",
    "\n",
    "Identify the types of information contained in our data set In this notebook I used Python modules to import external data sets for the purpose of getting to know/familiarize myself with the data to get a good grasp of the data and think about how to handle the data in different ways. \n",
    "\n",
    "--------------------------------------------------------\n",
    "\n",
    "### A. Import library\n",
    "\n",
    "First, let’s import all/ minimum of the modules, functions and objects we are going to use in this tutorial\n",
    "\n",
    "\n",
    "\n",
    "### B. Load Dataset\n",
    "\n",
    "First, load the supplied CSV file using additional options in the Pandas read_csv function.\n",
    "Inspecting the data or We can load the data directly from the UCI Machine Learning repository in sklearn.\n",
    "\n",
    "The first step is to visually inspect the new data set. There are multiple ways to achieve this:\n",
    "\n",
    "* The easiest being to request the first few records using the DataFrame data.head()* method. By default, “data.head()” returns the first 5 rows from the DataFrame object df (excluding the header row).\n",
    "* Alternatively, one can also use “df.tail()” to return the five rows of the data frame.\n",
    "* For both head and tail methods, there is an option to specify the number of records by including the required number in between the parentheses when calling either method.Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--libararies complate--\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "#---------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer Wisconsin (Diagnostic) Database\n",
      "=============================================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      "References\n",
      "----------\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "# showing data information\n",
    "# ----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "# 2. Data Explorations\n",
    "\n",
    "Goal : Explore the variables to assess how they relate to the response variable In this notebook, I am getting familiar with the data using data exploration and visualization techniques using python libraries (Pandas, matplotlib, seaborn. Familiarity with the data is important which will provide useful knowledge for data pre-processing)\n",
    "\n",
    "\n",
    "### Statistical Summary\n",
    "\n",
    "Now we can take a look at a summary of each attribute.\n",
    "\n",
    "This includes the count, mean, the min and max values as well as some percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use pandas as container the data\n",
    "# showing head of data\n",
    "# Set a global name X,y (for simple name)\n",
    "#---------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summirize dataset using matplotlib\n",
    "# Plot histograms \n",
    "#-------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "----------------------------------------\n",
    "# 3. Data Preprocessing\n",
    "\n",
    "Data preprocessing is a crucial step for any data analysis problem. It is often a very good idea to prepare your data in such way to best expose the structure of the problem to the machine learning algorithms that you intend to use.This involves a number of activities such as:\n",
    "- Assigning numerical values to categorical data;\n",
    "- Handling missing values; and\n",
    "- Normalizing the features (so that features on small scales do not dominate when fitting a model to the data).\n",
    "\n",
    "Goal : Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model. \n",
    "\n",
    "*) We assume that the dataset for this project is clean. So we not do preprocessing right now\n",
    "\n",
    "  \n",
    "--------------------------------\n",
    "\n",
    "In this sections to improve the models, I use  some techniques, that are : (Ignoring when you try a simple way)\n",
    "1. feature normalization\n",
    "2. feature selection to reduce high-dimension data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing code # \n",
    "# Feature normalize using StandardScaler()\n",
    "#---------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction using PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature selection and feature reduction using PCA\n",
    "#---------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking Data Representation\n",
    "# showing structure of Input (X)\n",
    "# showing structure of target (y)\n",
    "#---------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "----------------------------------------------------------\n",
    "# 4. Models Development \n",
    "\n",
    "In this section of the project, you will develop the tools and techniques necessary for models to make a prediction. Being able to make accurate evaluations of each model's performance through the use of these tools and techniques helps to greatly reinforce the confidence in your predictions.\n",
    "\n",
    "We don’t know which algorithms would be good on this problem or what configurations to use. We get an idea from the plots that some of the classes are partially linearly separable in some dimensions, so we are expecting generally good results.\n",
    "\n",
    "Let’s evaluate 3 different algorithms:\n",
    "- KNeighborsClassifier,\n",
    "- DecisionTreeClassifier,\n",
    "- RandomForestClassifier,\n",
    "- GaussianNB,     \n",
    "- SVM,\n",
    "\n",
    "\n",
    "----------------------------------------------------------\n",
    "### A. Split Data\n",
    "Before build the models, the next implementation requires that you take the dataset and split the data into training and testing subsets. Typically, the data is also shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset.\n",
    "\n",
    "For the code cell below, you will need to implement the following:\n",
    "\n",
    "- Use train_test_split from sklearn.cross_validation to shuffle and split the features and prices data into training and testing sets.\n",
    "- Split the data into 75% training and 25% testing.\n",
    "- Set the random_state for train_test_split to a value of your choice. This ensures results are consistent.\n",
    "    Assign the train and testing splits to X_train, X_test, y_train, and y_test.\n",
    "\n",
    "\n",
    "### B. Models Validation\n",
    "\n",
    "It is difficult to measure the quality of a given model without quantifying its performance over training and testing. This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement. For this project, we will be calculating the accuracy_score function to computes the accuracy. the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\n",
    "\n",
    "\n",
    "### C. Cross validation\n",
    "Cross Validation technique  assess the performance of machine learning models. It helps in knowing how the machine learning model would generalize to an independent data set. \n",
    "Tthis is one of resampling methods to make the best use of your training data in order to accurately estimate the performance of a model on new unseen data.\n",
    "\n",
    "\n",
    "Accurate estimates of performance can then be used to help you choose which set of model parameters to use or which model to select. Once you have chosen a model, you can train for final model on the entire training dataset and start using it to make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -Initialization method classifier\n",
    "# - Split data\n",
    "#---------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -Initialization method classifier\n",
    "# - Split data\n",
    "#---------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        accuracy  cross val stdv cross val time process\n",
      "KNeighborsClassifier    0.965035   0.926253      +/- 9.25%     0.036504\n",
      "DecisionTreeClassifier  0.944056   0.927976      +/- 6.54%     0.072742\n",
      "RandomForestClassifier  0.951049   0.959649      +/- 6.85%      0.25531\n",
      "GaussianNB              0.958042   0.936779      +/- 7.22%     0.013605\n",
      "SVC                     0.622378   0.627663     +/- 35.40%      0.80199\n"
     ]
    }
   ],
   "source": [
    "# Show evaluation to pandas\n",
    "#---------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "# 5. Model Improvement\n",
    "\n",
    "\n",
    "Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model. \n",
    "In this project I use two sub step to improve the accuracy the best model, that are: \n",
    "\n",
    "1. Preprocessing : We use feature selection to reduce high-dimension data, feature extraction and transformation for dimensionality reduction\n",
    "2. parameters tuning  in order to find one with the best model's performance with best parameters ( hyper-parameters). \n",
    "\n",
    "Note : Machine learning models are parameterized so that their behavior can be tuned for a given problem.\n",
    "Models can have many parameters and finding the best combination of parameters can be treated as a search problem. Not all parameters of a classifier is learned from the estimators. Those parameters are called hyper-parameters and are passed as arguments to the constructor of the classifier. Each estimator has a different set of hyper-parameters, which can be found in the corresponding documentation.\n",
    "We can search for the best performance of the classifier sampling different hyper-parameter combinations. This will be done with an exhaustive grid search, provided by the GridSearchCV function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "## SVM tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Now Trying Support Vector Machine Classifier ---------\n",
      "Support Vector Machine Accuracy: 97.20%\n",
      "Cross validation score: 93.83% (+/- 5.81%)\n",
      "Execution time: 2057.5 seconds \n",
      "\n",
      "Best parameters: {'kernel': 'linear', 'C': 0.1} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM tune parameters \n",
    "#---------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Now Trying Support Random Forest Classifier ---------\n",
      "Random Forest Accuracy: 96.50%\n",
      "Cross validation score: 95.62% (+/- 4.23%)\n",
      "Execution time: 57.645 seconds \n",
      "\n",
      "Best parameters: {'n_estimators': 91, 'criterion': 'entropy'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier tune parameters\n",
    "#--------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Now Trying Support Naive Bayes Classifier  ---------\n",
      "Accuracy: 95.80%\n",
      "Cross validation score: 93.69% (+/- 3.29%)\n",
      "Execution time: 0.24562 seconds \n",
      "\n",
      "Best parameters: {'priors': [0.4, 0.6]}\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier tune parameters\n",
    "#--------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "---------------------------\n",
    "-------------------------\n",
    "### Example : Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(data_scaler, 'data_scaler_cancer.pkl')\n",
    "# joblib.dump(pca, 'pca_cancer.pkl')\n",
    "# joblib.dump(clf_svc, 'svc_cancer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # load model:\n",
    "# # 1.standard\n",
    "# # 2. pca\n",
    "# # 3. algorithm\n",
    "# svm= joblib.load('svc_cancer.pkl')\n",
    "# predict=svm.predict(X_test)\n",
    "# print 'Score :',accuracy_score(predict,y_test)\n",
    "\n",
    "# svm= joblib.load('lr_cancer.pkl')\n",
    "# predict=svm.predict(X_test)\n",
    "# print 'Score :',accuracy_score(predict,y_test)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
