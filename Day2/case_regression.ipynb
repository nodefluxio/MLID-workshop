{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started \n",
    "In this project, you will evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a good fit could then be used to make certain predictions about a home — in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.\n",
    "\n",
    "The dataset for this project originates from the UCI Machine Learning Repository. The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts. For the purposes of this project, the following preoprocessing steps have been made to the dataset:\n",
    "\n",
    "+ 16 data points have an 'MDEV' value of 50.0. These data points likely contain missing or censored values and have been removed.\n",
    "+ 1 data point has an 'RM' value of 8.78. This data point can be considered an outlier and has been removed.\n",
    "+ The features 'RM', 'LSTAT', 'PTRATIO', and 'MDEV' are essential. The remaining non-relevant features have been excluded.\n",
    "+ The feature 'MDEV' has been multiplicatively scaled to account for 35 years of market inflation.\n",
    "\n",
    "Run the code cell below to load the Boston housing dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported.\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "## Objective:\n",
    "\n",
    "The repository is a learning exercise to:\n",
    "\n",
    "- Apply the fundamental concepts of machine learning from an available dataset\n",
    "\n",
    "The analysis is divided into four sections, saved in juypter notebooks in this repository\n",
    "\n",
    "1. Identifying the problem and Data Sources\n",
    "2. Data Explorations\n",
    "3. Data Pre-Processing\n",
    "4. Model Development\n",
    "5. Model Improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------\n",
    "# 1.  Identifying the problem and Getting data.\n",
    "\n",
    "Identify the types of information contained in our data set In this notebook I used Python modules to import external data sets for the purpose of getting to know/familiarize myself with the data to get a good grasp of the data and think about how to handle the data in different ways. \n",
    "\n",
    "--------------------------------------------------------\n",
    "\n",
    "### A. Import library\n",
    "\n",
    "First, let’s import all/ minimum of the modules, functions and objects we are going to use in this tutorial\n",
    "\n",
    "\n",
    "\n",
    "### B. Load Dataset\n",
    "\n",
    "First, load the supplied CSV file using additional options in the Pandas read_csv function.\n",
    "Inspecting the data or We can load the data directly from the UCI Machine Learning repository in sklearn.\n",
    "\n",
    "The first step is to visually inspect the new data set. There are multiple ways to achieve this:\n",
    "\n",
    "* The easiest being to request the first few records using the DataFrame data.head()* method. By default, “data.head()” returns the first 5 rows from the DataFrame object df (excluding the header row).\n",
    "* Alternatively, one can also use “df.tail()” to return the five rows of the data frame.\n",
    "* For both head and tail methods, there is an option to specify the number of records by including the required number in between the parentheses when calling either method.Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import library\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "dataset=load_boston()\n",
    "# data information\n",
    "print dataset.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------\n",
    "\n",
    "# 2. Data Explorations\n",
    "In this first section of this project, you will make a cursory investigation about the Boston housing data and provide your observations. Familiarizing yourself with the data through an explorative process is a fundamental practice to help you better understand and justify your results.\n",
    "\n",
    "### Statistical Summary\n",
    "\n",
    "Now we can take a look at a summary of each attribute.\n",
    "\n",
    "This includes the count, mean, the min and max values as well as some percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we use pandas as container the data\n",
    "# set to pandas\n",
    "pd_dataset=pd.DataFrame(dataset.data)\n",
    "pd_dataset.columns= dataset.feature_names\n",
    "\n",
    "# global name X,y for simple name\n",
    "X= pd_dataset\n",
    "y=dataset.target\n",
    "print pd_dataset.describe()\n",
    "print pd_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summirize dataset using matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "color_dic = {0:'red', 1:'blue'}\n",
    "#Plot histograms of CUT1 variables\n",
    "data_mean=pd_dataset.copy()\n",
    "data_mean['PRICE']=y\n",
    "\n",
    "plt.subplots(nrows=7, ncols=2, figsize=(10, 40))\n",
    "\n",
    "for i, col in enumerate(data_mean.columns):\n",
    "    plt.subplot(8, 2, i+1)\n",
    "    plt.plot(data_mean[col], data_mean['PRICE'], 'o')\n",
    "    plt.plot(np.unique(data_mean[col]), np.poly1d(np.polyfit(data_mean[col], data_mean['PRICE'], 1))(np.unique(data_mean[col])))\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('prices')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------------------\n",
    "# 3. Data Preprocessing\n",
    "\n",
    "Data preprocessing is a crucial step for any data analysis problem. It is often a very good idea to prepare your data in such way to best expose the structure of the problem to the machine learning algorithms that you intend to use.This involves a number of activities such as:\n",
    "- Assigning numerical values to categorical data;\n",
    "- Handling missing values; and\n",
    "- Normalizing the features (so that features on small scales do not dominate when fitting a model to the data).\n",
    "\n",
    "Goal : Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model. \n",
    "\n",
    "*) We assume that the dataset for this project is clean. So we not do preprocessing right now\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "In this sections to improve the models, I use  some techniques, that are :(Ignoring when you try a simple way)\n",
    "1. feature selection Using intuition\n",
    "2. feature normalization\n",
    "3. feature selection to reduce high-dimension data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection Using intuition\n",
    "\n",
    "based on intuition, we will use three features from the Boston housing dataset: 'RM', 'LSTAT', and 'PTRATIO'. For each data point (neighborhood):\n",
    "\n",
    "- 'RM' is the average number of rooms among homes in the neighborhood.\n",
    "-  'LSTAT' is the percentage of all Boston homeowners who have a greater net worth than homeowners in the neighborhood.\n",
    "- 'PTRATIO' is the ratio of students to teachers in primary and secondary schools in the neighborhood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X=pd_dataset.loc[:,['RM','PTRATIO','LSTAT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # # preprocessing code # Data pre-processing\n",
    "# # #---------------------------------------------\n",
    "\n",
    "# #feature normalizetion\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# data_scaler=StandardScaler()\n",
    "# X=data_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction using PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # feature selection and feature reduction\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# import numpy as np\n",
    "# # feature extraction\n",
    "# pca = PCA(n_components=10)\n",
    "# X_reduc = pca.fit_transform(Xs)\n",
    "\n",
    "# # summarize components\n",
    "# print(\"Explained Variance: %s\") % (sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# #The amount of variance that each PC explains\n",
    "# var= pca.explained_variance_ratio_\n",
    "# #Cumulative Variance explains\n",
    "# var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)              \n",
    "# print(var1)\n",
    "# X=X_reduc\n",
    "\n",
    "# # Deciding How Many Principal Components to Retain\n",
    "# # - Kaiser’s criterion,Elbow\n",
    "# # - screeplot\n",
    "# plt.plot(pca.explained_variance_)\n",
    "# plt.title('Scree Plot')\n",
    "# plt.xlabel('Principal Component')\n",
    "# plt.ylabel('Eigenvalue')\n",
    "\n",
    "# leg = plt.legend(['Eigenvalues from PCA'], loc='best', borderpad=0.3,shadow=False,markerscale=0.4)\n",
    "# leg.get_frame().set_alpha(0.4)\n",
    "# leg.draggable(state=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "# 4. Models Development \n",
    "\n",
    "In this section of the project, you will develop the tools and techniques necessary for models to make a prediction. Being able to make accurate evaluations of each model's performance through the use of these tools and techniques helps to greatly reinforce the confidence in your predictions.\n",
    "\n",
    "We don’t know which algorithms would be good on this problem or what configurations to use. We get an idea from the plots that some of the classes are partially linearly separable in some dimensions, so we are expecting generally good results.\n",
    "\n",
    "Let’s evaluate 3 different algorithms:\n",
    "- Gradient Boosting Regressor\n",
    "- Linear Regression\n",
    "- SVM\n",
    "\n",
    "----------------------------------------------------------\n",
    "### A. Split Data\n",
    "Before build the models, the next implementation requires that you take the dataset and split the data into training and testing subsets. Typically, the data is also shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset.\n",
    "\n",
    "For the code cell below, you will need to implement the following:\n",
    "\n",
    "- Use train_test_split from sklearn.cross_validation to shuffle and split the features and prices data into training and testing sets.\n",
    "- Split the data into 70% training and 30% testing.\n",
    "- Set the random_state for train_test_split to a value of your choice. This ensures results are consistent.\n",
    "    Assign the train and testing splits to X_train, X_test, y_train, and y_test.\n",
    "\n",
    "\n",
    "### B. Models Validation\n",
    "\n",
    "It is difficult to measure the quality of a given model without quantifying its performance over training and testing. This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement. For this project, we will be calculating the coefficient of determination(R2) and Mean square Error to quantify our model's performance. \n",
    "+ The coefficient of determination for a model is a useful statistic in regression analysis, as it often describes how \"good\" that model is at making predictions.The values for R2 range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the target variable\n",
    "+ The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. The squaring is necessary to remove any negative signs. It also gives more weight to larger differences. It’s called the mean squared error as you’re finding the average of a set of errors.\n",
    "\n",
    "\n",
    "\n",
    "### C. Cross validation\n",
    "Cross Validation technique  assess the performance of machine learning models. It helps in knowing how the machine learning model would generalize to an independent data set. \n",
    "Tthis is one of resampling methods to make the best use of your training data in order to accurately estimate the performance of a model on new unseen data.\n",
    "\n",
    "\n",
    "Accurate estimates of performance can then be used to help you choose which set of model parameters to use or which model to select. Once you have chosen a model, you can train for final model on the entire training dataset and start using it to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "#we use most of the algorithms for regression algorithm\n",
    "regressors=[GradientBoostingRegressor(),\n",
    "            LinearRegression(),\n",
    "            SVR()    \n",
    "           ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "\n",
    "# initalization validation score for reports\n",
    "valid_scores={'MSE':[],'R2':[], 'k-fold':[]}\n",
    "methods=[]\n",
    "\n",
    "for clf in regressors:\n",
    "    clf.fit(X_train, y_train)    \n",
    "    predict_score=clf.predict(X_test)\n",
    "    kfold = KFold(n_splits=10, random_state=42)\n",
    "    scores_fold = cross_val_score(clf, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    name= clf.__class__.__name__\n",
    "#     print (name)\n",
    "#     print y_test[0:5]\n",
    "#     print predict_score[0:5]\n",
    "    methods.append(name)\n",
    "#     print clf.score(y_test, predict_score)\n",
    "    valid_scores['MSE'].append(mean_squared_error(y_test, predict_score))  \n",
    "    valid_scores['R2'].append(r2_score(y_test, predict_score))\n",
    "    valid_scores['k-fold'].append((np.mean(scores_fold)))\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show evaluation\n",
    "pd_score=pd.DataFrame(valid_scores)\n",
    "pd_score.index=methods\n",
    "print pd_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "# 5. Model Improvement\n",
    "\n",
    "\n",
    "Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model. \n",
    "In this project I use two sub step to improve the accuracy the best model, that are: \n",
    "\n",
    "1. Preprocessing : We use feature selection to reduce high-dimension data, feature extraction and transformation for dimensionality reduction\n",
    "2. parameters tuning  in order to find one with the best model's performance with best parameters ( hyper-parameters). \n",
    "\n",
    "Note : Machine learning models are parameterized so that their behavior can be tuned for a given problem.\n",
    "Models can have many parameters and finding the best combination of parameters can be treated as a search problem. Not all parameters of a classifier is learned from the estimators. Those parameters are called hyper-parameters and are passed as arguments to the constructor of the classifier. Each estimator has a different set of hyper-parameters, which can be found in the corresponding documentation.\n",
    "We can search for the best performance of the classifier sampling different hyper-parameter combinations. This will be done with an exhaustive grid search, provided by the GridSearchCV function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#code Gridsearch here\n",
    "\n",
    "print('--------- Now Trying Gradient Boosting Regressor ---------')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gr = GradientBoostingRegressor()\n",
    "\n",
    "# Finally experiment with SVR\n",
    "param_grid={'n_estimators':[100], \n",
    "            'learning_rate': [0.1,0.05, 0.02, 0.01],            \n",
    "            'max_depth':[4,6,7,9], \n",
    "            'min_samples_leaf':[3,5,7], \n",
    "            'max_features':[1.0,0.3,0.1]\n",
    "           } \n",
    "            \n",
    "\n",
    "# cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n",
    "\n",
    "grid_search = GridSearchCV(gr, param_grid, cv=10, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "gr_best = grid_search.best_estimator_\n",
    "# print grid_search.best_score_\n",
    "\n",
    "predictions_gr = gr_best.predict(X_test)\n",
    "\n",
    "print('MSE: {0:.3f}'.format(mean_squared_error(y_test, predictions_gr)))\n",
    "print('MAE: {0:.3f}'.format(mean_absolute_error(y_test, predictions_gr)))\n",
    "print('R^2: {0:.3f}'.format(r2_score(y_test, predictions_gr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#code Gridsearch here\n",
    "\n",
    "print('--------- Now Trying SVR ---------')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svr = SVR(kernel='linear')\n",
    "\n",
    "# Finally experiment with SVR\n",
    "param_grid = [\n",
    "        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n",
    "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]}]\n",
    "        \n",
    "\n",
    "# cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n",
    "\n",
    "grid_search = GridSearchCV(svr, param_grid, cv=10, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "svr_best = grid_search.best_estimator_\n",
    "# print grid_search.best_score_\n",
    "\n",
    "predictions_svr = svr_best.predict(X_test)\n",
    "\n",
    "print('MSE: {0:.3f}'.format(mean_squared_error(y_test, predictions_svr)))\n",
    "print('MAE: {0:.3f}'.format(mean_absolute_error(y_test, predictions_svr)))\n",
    "print('R^2: {0:.3f}'.format(r2_score(y_test, predictions_svr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
